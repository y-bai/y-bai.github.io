---
title: "Word2vec"
date: 2023-05-20 10:14:00 +0800
categories: [NLP]
tags: [Embedding]
---

# 什么是word2vec嵌入? 
word2vec 模型是一个<kbd>仅仅包括一层隐含层</kbd>的MLP网络。
- word2vec的任务是预测在一个句子中的`context`中与当前word近邻的words. 
- 但是，word2vec模型的目标函数与该任务没有任何关系。所有我们想要得到的是<kbd>隐含层学习到的权重矩阵</kbd>，这个权重矩阵就是我们用来作为`词嵌入(word embeddings)`。 这其实是一种在非监督学习中常见的*trick*。比如训练`自动编码器`的过程是在隐藏层压缩降维输入向量，并在输出层将其解压缩回原始向量。训练完成后，剥离并去除输出层（去除解压步骤），只使用隐藏层--这是在没有标记训练数据的情况下学习良好低维特征的技巧。

word2vec 模型包括两种变种：**Continuous Bag of Words** 和 **Skip-Gram model**。 我们主要说明Skip-Gram model.

![word2vec model architecture]({{ "/assets/img/blogs/word2vec-skip-gram1.png" | relative_url }}) 





